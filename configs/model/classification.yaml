# Classification model config
# Encoder is injected via model/net group (e.g., override /model/net: powsm or ipa_embedding)
# Head is injected via model/head group (e.g., override /model/head: transformer or rnn)
# Head is built partially here. The input dimension for head is inferred dynamically based on the encoder.
# Two input modes are supported:
# 1. Audio mode (input_type: "audio"): Uses audio encoder (powsm, wav2vec2phoneme)
#    - Data batch contains: speech, speech_length, label
# 2. IPA mode (input_type: "ipa"): Uses IPA embedding
#    - Data batch contains: ipa_ids, lengths, label

defaults:
  - head: attnmlp

_target_: src.recipe.common.classification_module.ClassificationModel

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.LinearLR
  _partial_: true
  total_iters: 200
  last_epoch: -1

# Encoder must be provided via model/net group
net: ???
# Head configuration is loaded from configs/model/head/*.yaml
head:
  output_dim: ${data.num_classes}
  # partially constructed here

num_classes: ${data.num_classes}
freeze_encoder: false  # True/False for audio mode; false for IPA mode
input_type: "audio" # Input mode: "audio" for audio encoder, "ipa" for IPA embedding