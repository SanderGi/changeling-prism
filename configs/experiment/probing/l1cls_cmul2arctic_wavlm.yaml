# @package _global_
#
# to execute this experiment run:
# python src/main.py experiment=probing/l1cls_cmul2arctic_wavlm
#

defaults:
  - override /data: cmul2arcticl1
  - override /model: classification
  - override /model/head: attnmlp
  - override /model/net: wavlm
  - override /trainer: gpu
  - override /callbacks: default
  - override /logger: wandb

task_name: "l1cls_cmul2arctic_wavlm"
tags: ["cmul2arctic", "wavlm", "l1cls_fix"]

seed: 42

trainer:
  min_epochs: 10
  max_epochs: 50 # NOTE(shikhar): 10 epochs are enough, this model overfits after that
  gradient_clip_val: 1.0
  val_check_interval: 0.25 # validate 4 times per epoch
  callbacks:
    - learning_rate_monitor

model:
  head:
    task_type: "classification"
  freeze_encoder: true
  optimizer:
    lr: 0.0002
  scheduler:
    total_iters: 200
  net:
    hf_repo: microsoft/wavlm-base
    output_vocabsz: null

callbacks:
  model_checkpoint:
    monitor: "val/f1"
    mode: "max"
    save_top_k: 0
  early_stopping:
    monitor: "val/f1"
    patience: 5
    mode: "max"

data:
  batch_size: 48 # 48 on delta-ai, 24 on delta (1gpu)

logger:
  wandb:
    tags: ${tags}
    group: "l1cls_cmul2arctic_wavlm"
    name: ${task_name}
