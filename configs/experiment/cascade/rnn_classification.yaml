# @package _global_

# run transcript --> label classification training with rnn
# To execute this experiment run:
# python src/main.py experiment=cascade/rnn_cls.yaml

# To use different IPA sources, override data.json_path:
# python src/main.py experiment=cascade/rnn_cls.yaml data.json_path=/work/nvme/bbjs/sbharadwaj/powsm/PRiSM/exp/runs/w2v2ph_transcription/20251209_111713/transcription.json

defaults:
  - override /data: transcript_dataset
  - override /model: classification
  - override /model/net: ipa_embedding
  - override /model/head: rnn
  - override /trainer: gpu
  - override /callbacks: default
  - override /logger: wandb

task_name: "rnn_cascade"
tags: ["cascade","rnn","classification"]

seed: 42

trainer:
  min_epochs: 10
  max_epochs: 200
  gradient_clip_val: 1.0
  val_check_interval: 0.25  # validate 4 times per epoch

data:
  json_path: ???
  batch_size: 64
  num_workers: 4

model:
  head:
    task_type: "classification"
  input_type: "ipa"  # Use IPA text input mode
  freeze_encoder: false  # IPA embedding is trainable
  encoder_output_dim: 128  # Must match ipa_embedding.embedding_dim
  optimizer:
    lr: 0.001
  # Override net config to set vocab_size
  # Note: vocab_size will be determined dynamically from the datamodule
  # For now, set a reasonable upper bound (can be overridden)
  net:
    vocab_size: 200
    embedding_dim: ${model.encoder_output_dim}

callbacks:
  model_checkpoint:
    monitor: "val/f1"
    mode: "max"
    save_top_k: 1
  early_stopping:
    monitor: "val/f1"
    patience: 100
    mode: "max"

logger:
  wandb:
    tags: ${tags}
    group: ${task_name}
    name: ${task_name}